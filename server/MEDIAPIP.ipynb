{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "891eadc8-5163-4b2e-bc44-57ffd5157fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Installed mediapipe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -q mediapipe\n",
    "print(\"Installed mediapipe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91274810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "הגרסה הנכונה של configuring_Mediapipe_module.py נטענה\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m arrMatchingTraits\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     20\u001b[0m arrf \u001b[38;5;241m=\u001b[39m [f1,f2,f3,f5,f6,f7,f8,f9,f10,f11,f12,f13,f14,f15,f16,f17]\n\u001b[1;32m---> 21\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mface_reconize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m detection_result \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect(image)\n\u001b[0;32m     23\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m draw_landmarks_on_image(image\u001b[38;5;241m.\u001b[39mnumpy_view(), detection_result)\n",
      "File \u001b[1;32mc:\\Users\\This User\\Desktop\\פרויקט גמר\\server\\yolo_and_taking_picture.py:31\u001b[0m, in \u001b[0;36mface_reconize\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mface_reconize\u001b[39m(image):\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m yolo_detect_valid_face(\u001b[43mframe\u001b[49m):\n\u001b[0;32m     32\u001b[0m       timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m       image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaptured_image_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'frame' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import importlib\n",
    "import facial_features_functions\n",
    "# importlib.reload(facial_features_functions)\n",
    "from facial_features_functions import f1, f2, f3, f5, f6, f7, f8,f9, f10, f11, f12, f13, f14, f15, f16, f17\n",
    "import configuring_Mediapipe_module\n",
    "importlib.reload(configuring_Mediapipe_module)\n",
    "from configuring_Mediapipe_module import draw_landmarks_on_image, detector\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import yolo_and_taking_picture\n",
    "importlib.reload(yolo_and_taking_picture)\n",
    "from yolo_and_taking_picture import face_reconize\n",
    "from dictionary_definition import my_dict\n",
    "arrMatchingTraits=[]\n",
    "arrf = [f1,f2,f3,f5,f6,f7,f8,f9,f10,f11,f12,f13,f14,f15,f16,f17]\n",
    "image = face_reconize()\n",
    "detection_result = detector.detect(image)\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "cv2.imshow(\"Annotated Image\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "for i in range(0,16):\n",
    "    arrMatchingTraits.append(arrf[i](detection_result))\n",
    "# ניתוח אישיות\n",
    "with open(\"personality_data.json\", encoding=\"utf-8\") as f:\n",
    "    personality_data=json.load(f)\n",
    "a=0b00000000\n",
    "count=0\n",
    "i=1\n",
    "while count <4: # חיפוש 4 תכונות\n",
    "    if i==8:\n",
    "        break\n",
    "    v1 = 0 \n",
    "    v2=0 # אתחול בכל איטרציה\n",
    "    for j in personality_data[str(i)][\"values1\"]:\n",
    "        v1 += arrMatchingTraits[j]# חיבור התכונות\n",
    "    for j in personality_data[str(i)][\"values2\"]:\n",
    "        v1 +=(100-arrMatchingTraits[j])# חיבור התכונות\n",
    "    v1/=personality_data[str(i)][\"count\"]\n",
    "    print(\"v1\",v1)\n",
    "    for j in personality_data[str(i+1)][\"values1\"]:\n",
    "        v2 += arrMatchingTraits[j]# חיבור התכונותv2 += arrMatchingTraits[j] # חיבור התכונות\n",
    "    for j in personality_data[str(i+1)][\"values2\"]:\n",
    "        v2 +=(100-arrMatchingTraits[j])# חיבור התכונותv2 += arrMatchingTraits[j] # חיבור התכונות\n",
    "    v2/=personality_data[str(i+1)][\"count\"]\n",
    "    print(\"v2\",v2)\n",
    "    a|=int(personality_data[str(i)][\"name\"],2) if v1>v2 else int(personality_data[str(i+1)][\"name\"],2) # חיבור השמות\n",
    "    print(\"a\",bin(a))\n",
    "    i=i+2\n",
    "    count=count+1 # עדכון משתנה count\n",
    "print(\"מאפייני האישיות:\", my_dict[a])\n",
    "# print(\"arrMatchingTraits =\", arrMatchingTraits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66b2d425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tkinter as tk\n",
    "# import mediapipe as mp\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import datetime\n",
    "# import json\n",
    "# from tkinter import filedialog\n",
    "# from ultralytics import YOLO\n",
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "# from facial_features_functions import f1, f2, f3, f5, f6, f7, f8\n",
    "# from mediapipe import solutions\n",
    "# from mediapipe.framework.formats import landmark_pb2\n",
    "# arrMatchingTraits=[]\n",
    "# arrf = [f1, f2, f3, f5, f6, f7, f8]\n",
    "# # YOLO model trained to detect \"face straight, mouth closed\"\n",
    "# yolo_model = YOLO(r\"C:\\Users\\This User\\Desktop\\try\\best.pt\")\n",
    "# # פונקציה לבדוק אם YOLO זיהה פנים מתאימות\n",
    "# def yolo_detect_valid_face(image):\n",
    "#     results = yolo_model.predict(source=image, conf=0.27, save=False, verbose=False)\n",
    "#     if results and results[0].boxes:\n",
    "#         names = results[0].names\n",
    "#         detected_labels = set()\n",
    "#         for box in results[0].boxes:\n",
    "#             cls_id = int(box.cls[0])\n",
    "#             label = names[cls_id]\n",
    "#             detected_labels.add(label)\n",
    "#         required_labels = {\"Straight face\",\"Looking to camera\",\"Closed mouth\"}\n",
    "#         return required_labels.issubset(detected_labels)\n",
    "#     return False\n",
    "# # בחירת תמונה או צילום\n",
    "\n",
    "# def face_reconize():\n",
    "#     print(\"בחר אחת מהאפשרויות:\")\n",
    "#     print(\"1 - צילום תמונה\")\n",
    "#     print(\"2 - העלאת תמונה מהמחשב\")\n",
    "#     choice = input(\"הכנס את מספר הבחירה: \")\n",
    "#     if choice == \"1\":\n",
    "#         save_dir = \"saved_images\"\n",
    "#         os.makedirs(save_dir, exist_ok=True)\n",
    "#         cap = cv2.VideoCapture(0)\n",
    "#         if not cap.isOpened():\n",
    "#             print(\"לא מצליח לפתוח מצלמה.\")\n",
    "#             exit()\n",
    "#         print(\"מצלם... לחץ 'q' כדי לצאת.\")\n",
    "#         saved = False\n",
    "#         while True:\n",
    "#             ret, frame = cap.read()\n",
    "#             if not ret:\n",
    "#                 break\n",
    "#             if yolo_detect_valid_face(frame):\n",
    "#                 timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "#                 image_path = os.path.join(save_dir, f\"captured_image_{timestamp}.png\")\n",
    "#                 cv2.imwrite(image_path, frame)\n",
    "#                 print(f\"התמונה נשמרה: {image_path}\")\n",
    "#                 saved = True\n",
    "#                 break\n",
    "#             cv2.imshow(\"Camera\", frame)\n",
    "#             if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "#         if not saved:\n",
    "#             print(\"לא נשמרה תמונה – לא נמצאו פנים מתאימות.\")\n",
    "#             exit()\n",
    "#         frame = cv2.imread(image_path)\n",
    "#         image = mp.Image(mp.ImageFormat.SRGB, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "#     elif choice == \"2\":\n",
    "#         root = tk.Tk()\n",
    "#         root.withdraw()\n",
    "#         file_path = filedialog.askopenfilename(filetypes=[(\"Image files\", \"*.jpg *.jpeg *.png\")])\n",
    "#         if not file_path:\n",
    "#             print(\"לא נבחר קובץ.\")\n",
    "#             exit()\n",
    "#         frame = cv2.imread(file_path)\n",
    "#         if frame is None:\n",
    "#             print(\"שגיאה בטעינת הקובץ.\")\n",
    "#             exit()\n",
    "#         if not yolo_detect_valid_face(frame):\n",
    "#             print(\"התמונה לא עומדת בתנאים (פנים לא ישרות/פה פתוח).\")\n",
    "#             exit()\n",
    "#         image = mp.Image(mp.ImageFormat.SRGB, cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "#     else:\n",
    "#         print(\"בחירה לא חוקית.\")\n",
    "#         exit()\n",
    "#     return image\n",
    "# # זיהוי נקודות פנים עם MediaPipe\n",
    "# image = face_reconize()\n",
    "# detection_result = detector.detect(image)\n",
    "# annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "# # הצגת התמונה עם הנקודות\n",
    "# cv2.imshow(\"Annotated Image\", cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n",
    "# # כאן אפשר להוסיף ציור נקודות אם יש לך draw_landmarks_on_image\n",
    "# # חישוב תכונות פנים\n",
    "# for i in range(0,7):\n",
    "#     arrMatchingTraits.append(arrf[i](detection_result))\n",
    "# # ניתוח אישיות\n",
    "# with open(\"personality_data.json\", encoding=\"utf-8\") as f:\n",
    "#     personality_data=json.load(f)\n",
    "# a=0b00000000\n",
    "# count=0\n",
    "# i=1\n",
    "# while count <4: # חיפוש 4 תכונות\n",
    "#     if i==8:\n",
    "#         break\n",
    "#     v1 = 0 \n",
    "#     v2=0 # אתחול בכל איטרציה\n",
    "#     for j in personality_data[str(i)][\"values\"]:\n",
    "#         v1 += arrMatchingTraits[j]# חיבור התכונות\n",
    "#     v1/=personality_data[str(i)][\"count\"]\n",
    "#     print(\"v1\",v1)\n",
    "#     for j in personality_data[str(i+1)][\"values\"]:\n",
    "#         v2 += arrMatchingTraits[j] # חיבור התכונות\n",
    "#     v2/=personality_data[str(i+1)][\"count\"]\n",
    "#     print(\"v2\",v2)\n",
    "#     a|=int(personality_data[str(i)][\"name\"],2) if v1>v2 else int(personality_data[str(i+1)][\"name\"],2) # חיבור השמות\n",
    "#     print(\"a\",bin(a))\n",
    "#     i=i+2\n",
    "#     count=count+1 # עדכון משתנה count\n",
    "# print(\"מאפייני האישיות:\", my_dict[a])\n",
    "# print(\"arrMatchingTraits =\", arrMatchingTraits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f58c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #חישוב ועדכון הממוצעים\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import mediapipe as mp\n",
    "# # חישוב ממוצעים של תכונות פנים\n",
    "# from facial_features_functions import f1, f2, f3, f4, f5, f6, f7, f8\n",
    "# arrMatchingTraits = []\n",
    "# arrf =[f1,f2,f3,f4,f5,f6,f7,f8]\n",
    "# j=0\n",
    "# # להפוך למטריצה\n",
    "# arrFase=[]\n",
    "# for i in range(0,16):\n",
    "#   for j in range(0,2):\n",
    "#   # נתיב לתיקייה עם תמונות (יש לעדכן את הנתיב הרצוי)\n",
    "#     input_folder = f'C:\\\\Users\\\\This User\\\\Desktop\\\\photos to project\\\\{arrFase[i][j]}'\n",
    "#     os.makedirs(output_folder, exist_ok=True)  # יצירת תיקיית פלט אם לא קיימת\n",
    "#     # מעבר על כל התמונות בתיקייה\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#        arr=[]\n",
    "#        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):  # סינון קבצים שהם תמונות\n",
    "#           input_path = os.path.join(input_folder, filename)\n",
    "#           output_path = os.path.join(output_folder, f\"annotated_{filename}\")\n",
    "#           # קריאת התמונה\n",
    "#           frame = cv2.imread(input_path)\n",
    "#           if frame is None:\n",
    "#               print(f\"Warning: Could not read {filename}, skipping.\")\n",
    "#               continue\n",
    "#           # המרת צבעים ל-RGB לעיבוד עם MediaPipe\n",
    "#           frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#           image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame_rgb)\n",
    "#           # זיהוי נקודות פנים\n",
    "#           detection_result = detector.detect(image)\n",
    "#           if detection_result.face_landmarks:\n",
    "#               # ציור הנקודות ושמירת התוצאה\n",
    "#               annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "#               cv2.imwrite(output_path, cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "#           b=arrf[i](detection_result)\n",
    "#           if b is not None:\n",
    "#               arr.append(b)\n",
    "#           else:\n",
    "#               arr.append(\"Face not detected\")\n",
    "#   avg= np.mean(arr)\n",
    "#   print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f92db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mediapipe.tasks import python\n",
    "# from mediapipe.tasks.python import vision\n",
    "# #  הגדרת מודול Mediapipe\n",
    "# def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "#     face_landmarks_list = detection_result.face_landmarks\n",
    "#     annotated_image = np.copy(rgb_image)\n",
    "#     for face_landmarks in face_landmarks_list:\n",
    "#         face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "#         face_landmarks_proto.landmark.extend([\n",
    "#             landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) \n",
    "#             for landmark in face_landmarks\n",
    "#         ])\n",
    "#         solutions.drawing_utils.draw_landmarks(\n",
    "#             image=annotated_image,\n",
    "#             landmark_list=face_landmarks_proto,\n",
    "#             connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "#             landmark_drawing_spec=None,\n",
    "#             connection_drawing_spec=mp.solutions.drawing_styles\n",
    "#             .get_default_face_mesh_tesselation_style())   \n",
    "#         solutions.drawing_utils.draw_landmarks(\n",
    "#             image=annotated_image,\n",
    "#             landmark_list=face_landmarks_proto,\n",
    "#             connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "#             landmark_drawing_spec=None,\n",
    "#             connection_drawing_spec=mp.solutions.drawing_styles\n",
    "#             .get_default_face_mesh_contours_style())\n",
    "#         solutions.drawing_utils.draw_landmarks(\n",
    "#             image=annotated_image,\n",
    "#             landmark_list=face_landmarks_proto,\n",
    "#             connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "#             landmark_drawing_spec=None,\n",
    "#             connection_drawing_spec=mp.solutions.drawing_styles\n",
    "#             .get_default_face_mesh_iris_connections_style()) \n",
    "#     return annotated_image\n",
    "# # MediaPipe model for facial landmarks\n",
    "# base_options = python.BaseOptions(model_asset_path=r'C:\\face_landmarker_v2_with_blendshapes.task')\n",
    "# options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "#                                        output_face_blendshapes=True,\n",
    "#                                        output_facial_transformation_matrixes=True,\n",
    "#                                        num_faces=1)\n",
    "# detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "339911a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to open file at c:\\Users\\This User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages/c:\\Users\\This User\\Desktop\\פרויקט גמר\\server\\face_landmarker_v2_with_blendshapes.task, errno=22",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 42\u001b[0m\n\u001b[0;32m     35\u001b[0m base_options \u001b[38;5;241m=\u001b[39m python\u001b[38;5;241m.\u001b[39mBaseOptions(model_asset_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface_landmarker_v2_with_blendshapes.task\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     36\u001b[0m options \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mFaceLandmarkerOptions(\n\u001b[0;32m     37\u001b[0m     base_options\u001b[38;5;241m=\u001b[39mbase_options,\n\u001b[0;32m     38\u001b[0m     output_face_blendshapes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m     output_facial_transformation_matrixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     40\u001b[0m     num_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     41\u001b[0m )\n\u001b[1;32m---> 42\u001b[0m detector \u001b[38;5;241m=\u001b[39m \u001b[43mvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFaceLandmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m mp_image \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mImage(image_format\u001b[38;5;241m=\u001b[39mmp\u001b[38;5;241m.\u001b[39mImageFormat\u001b[38;5;241m.\u001b[39mSRGB, data\u001b[38;5;241m=\u001b[39mrgb_image)\n\u001b[0;32m     45\u001b[0m detection_result \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect(mp_image)\n",
      "File \u001b[1;32mc:\\Users\\This User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\face_landmarker.py:3104\u001b[0m, in \u001b[0;36mFaceLandmarker.create_from_options\u001b[1;34m(cls, options)\u001b[0m\n\u001b[0;32m   3091\u001b[0m   output_streams\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m   3092\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([_FACE_GEOMETRY_TAG, _FACE_GEOMETRY_STREAM_NAME])\n\u001b[0;32m   3093\u001b[0m   )\n\u001b[0;32m   3095\u001b[0m task_info \u001b[38;5;241m=\u001b[39m _TaskInfo(\n\u001b[0;32m   3096\u001b[0m     task_graph\u001b[38;5;241m=\u001b[39m_TASK_GRAPH_NAME,\n\u001b[0;32m   3097\u001b[0m     input_streams\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3102\u001b[0m     task_options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   3103\u001b[0m )\n\u001b[1;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_graph_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3106\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable_flow_limiting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\n\u001b[0;32m   3107\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_RunningMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLIVE_STREAM\u001b[49m\n\u001b[0;32m   3108\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3109\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpackets_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_callback\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3111\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\This User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\mediapipe\\tasks\\python\\vision\\core\\base_vision_task_api.py:70\u001b[0m, in \u001b[0;36mBaseVisionTaskApi.__init__\u001b[1;34m(self, graph_config, running_mode, packet_callback)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m packet_callback:\n\u001b[0;32m     66\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     67\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe vision task is in image or video mode, a user-defined result \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     68\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback should not be provided.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     69\u001b[0m   )\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_runner \u001b[38;5;241m=\u001b[39m \u001b[43m_TaskRunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpacket_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_running_mode \u001b[38;5;241m=\u001b[39m running_mode\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Unable to open file at c:\\Users\\This User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages/c:\\Users\\This User\\Desktop\\פרויקט גמר\\server\\face_landmarker_v2_with_blendshapes.task, errno=22"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# --- טען תמונה לבדיקה ---\n",
    "image_path=r\"C:\\Users\\This User\\Desktop\\1.jpg\"\n",
    "bgr_image = cv2.imread(image_path)\n",
    "if bgr_image is None:\n",
    "    print(\"❌ לא הצלחנו לקרוא את התמונה מהנתיב:\")\n",
    "    print(image_path)\n",
    "    exit()\n",
    "rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# ========== 1. FaceMesh ==========\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True,\n",
    "                                   max_num_faces=1,\n",
    "                                   refine_landmarks=True,\n",
    "                                   min_detection_confidence=0.5)\n",
    "results_mesh = face_mesh.process(rgb_image)\n",
    "\n",
    "face_mesh_image = bgr_image.copy()\n",
    "if results_mesh.multi_face_landmarks:\n",
    "    for landmarks in results_mesh.multi_face_landmarks:\n",
    "        mp.solutions.drawing_utils.draw_landmarks(\n",
    "            image=face_mesh_image,\n",
    "            landmark_list=landmarks,\n",
    "            connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "# ========== 2. FaceLandmarker ==========\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=True,\n",
    "    num_faces=1\n",
    ")\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n",
    "\n",
    "mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_image)\n",
    "detection_result = detector.detect(mp_image)\n",
    "\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "\n",
    "face_landmarker_image = bgr_image.copy()\n",
    "for face_landmarks in detection_result.face_landmarks:\n",
    "    face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    face_landmarks_proto.landmark.extend([\n",
    "        landmark_pb2.NormalizedLandmark(x=l.x, y=l.y, z=l.z) for l in face_landmarks\n",
    "    ])\n",
    "    mp.solutions.drawing_utils.draw_landmarks(\n",
    "        image=face_landmarker_image,\n",
    "        landmark_list=face_landmarks_proto,\n",
    "        connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "        landmark_drawing_spec=None,\n",
    "        connection_drawing_spec=mp.solutions.drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "# ========== הצגת התוצאה ==========\n",
    "cv2.imshow(\"FaceMesh (refine_landmarks)\", face_mesh_image)\n",
    "cv2.imshow(\"FaceLandmarker\", face_landmarker_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
